{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import all necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/flatironschool/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/flatironschool/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#!pip install seaborn\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import nltk\n",
    "import sklearn\n",
    "import nltk.collocations \n",
    "from nltk import FreqDist, word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import string, re\n",
    "import urllib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('words')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import svm\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read CSV Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_list = pd.read_csv('380lyrics.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Rows with Other and Not Available Genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Rock             131377\n",
       "Pop               49444\n",
       "Hip-Hop           33965\n",
       "Not Available     29814\n",
       "Metal             28408\n",
       "Other             23683\n",
       "Country           17286\n",
       "Jazz              17147\n",
       "Electronic        16205\n",
       "R&B                5935\n",
       "Indie              5732\n",
       "Folk               3241\n",
       "Name: genre, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "song_list.genre.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(299523, 6)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "song_list.drop(song_list[song_list.genre == \"Other\"].index, inplace = True)\n",
    "song_list.drop(song_list[song_list.genre == \"Not Available\"].index, inplace = True)\n",
    "song_list.drop(song_list[song_list.genre == \"Indie\"].index, inplace = True)\n",
    "song_list.drop(song_list[song_list.genre == \"Folk\"].index, inplace = True)\n",
    "song_list.drop(song_list[song_list.artist == \"dolcenera\"].index, inplace = True)\n",
    "song_list.drop(song_list[song_list.artist == \"brthhse-onkelz\"].index, inplace = True)\n",
    "song_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>song</th>\n",
       "      <th>year</th>\n",
       "      <th>artist</th>\n",
       "      <th>genre</th>\n",
       "      <th>lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>362232</th>\n",
       "      <td>362232</td>\n",
       "      <td>who-am-i-drinking-tonight</td>\n",
       "      <td>2012</td>\n",
       "      <td>edens-edge</td>\n",
       "      <td>Country</td>\n",
       "      <td>I gotta say\\nBoy, after only just a couple of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362233</th>\n",
       "      <td>362233</td>\n",
       "      <td>liar</td>\n",
       "      <td>2012</td>\n",
       "      <td>edens-edge</td>\n",
       "      <td>Country</td>\n",
       "      <td>I helped you find her diamond ring\\nYou made m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362234</th>\n",
       "      <td>362234</td>\n",
       "      <td>last-supper</td>\n",
       "      <td>2012</td>\n",
       "      <td>edens-edge</td>\n",
       "      <td>Country</td>\n",
       "      <td>Look at the couple in the corner booth\\nLooks ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362235</th>\n",
       "      <td>362235</td>\n",
       "      <td>christ-alone-live-in-studio</td>\n",
       "      <td>2012</td>\n",
       "      <td>edens-edge</td>\n",
       "      <td>Country</td>\n",
       "      <td>When I fly off this mortal earth\\nAnd I'm meas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362236</th>\n",
       "      <td>362236</td>\n",
       "      <td>amen</td>\n",
       "      <td>2012</td>\n",
       "      <td>edens-edge</td>\n",
       "      <td>Country</td>\n",
       "      <td>I heard from a friend of a friend of a friend ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         index                         song  year      artist    genre  \\\n",
       "362232  362232    who-am-i-drinking-tonight  2012  edens-edge  Country   \n",
       "362233  362233                         liar  2012  edens-edge  Country   \n",
       "362234  362234                  last-supper  2012  edens-edge  Country   \n",
       "362235  362235  christ-alone-live-in-studio  2012  edens-edge  Country   \n",
       "362236  362236                         amen  2012  edens-edge  Country   \n",
       "\n",
       "                                                   lyrics  \n",
       "362232  I gotta say\\nBoy, after only just a couple of ...  \n",
       "362233  I helped you find her diamond ring\\nYou made m...  \n",
       "362234  Look at the couple in the corner booth\\nLooks ...  \n",
       "362235  When I fly off this mortal earth\\nAnd I'm meas...  \n",
       "362236  I heard from a friend of a friend of a friend ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "song_list.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop rows with NAN values for column song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "song_list.dropna(inplace = True)\n",
    "song_list.song.isna().sum()\n",
    "song_list.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_0</th>\n",
       "      <th>index</th>\n",
       "      <th>song</th>\n",
       "      <th>year</th>\n",
       "      <th>artist</th>\n",
       "      <th>genre</th>\n",
       "      <th>lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>231786</th>\n",
       "      <td>362232</td>\n",
       "      <td>362232</td>\n",
       "      <td>who-am-i-drinking-tonight</td>\n",
       "      <td>2012</td>\n",
       "      <td>edens-edge</td>\n",
       "      <td>Country</td>\n",
       "      <td>I gotta say\\nBoy, after only just a couple of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231787</th>\n",
       "      <td>362233</td>\n",
       "      <td>362233</td>\n",
       "      <td>liar</td>\n",
       "      <td>2012</td>\n",
       "      <td>edens-edge</td>\n",
       "      <td>Country</td>\n",
       "      <td>I helped you find her diamond ring\\nYou made m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231788</th>\n",
       "      <td>362234</td>\n",
       "      <td>362234</td>\n",
       "      <td>last-supper</td>\n",
       "      <td>2012</td>\n",
       "      <td>edens-edge</td>\n",
       "      <td>Country</td>\n",
       "      <td>Look at the couple in the corner booth\\nLooks ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231789</th>\n",
       "      <td>362235</td>\n",
       "      <td>362235</td>\n",
       "      <td>christ-alone-live-in-studio</td>\n",
       "      <td>2012</td>\n",
       "      <td>edens-edge</td>\n",
       "      <td>Country</td>\n",
       "      <td>When I fly off this mortal earth\\nAnd I'm meas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231790</th>\n",
       "      <td>362236</td>\n",
       "      <td>362236</td>\n",
       "      <td>amen</td>\n",
       "      <td>2012</td>\n",
       "      <td>edens-edge</td>\n",
       "      <td>Country</td>\n",
       "      <td>I heard from a friend of a friend of a friend ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        level_0   index                         song  year      artist  \\\n",
       "231786   362232  362232    who-am-i-drinking-tonight  2012  edens-edge   \n",
       "231787   362233  362233                         liar  2012  edens-edge   \n",
       "231788   362234  362234                  last-supper  2012  edens-edge   \n",
       "231789   362235  362235  christ-alone-live-in-studio  2012  edens-edge   \n",
       "231790   362236  362236                         amen  2012  edens-edge   \n",
       "\n",
       "          genre                                             lyrics  \n",
       "231786  Country  I gotta say\\nBoy, after only just a couple of ...  \n",
       "231787  Country  I helped you find her diamond ring\\nYou made m...  \n",
       "231788  Country  Look at the couple in the corner booth\\nLooks ...  \n",
       "231789  Country  When I fly off this mortal earth\\nAnd I'm meas...  \n",
       "231790  Country  I heard from a friend of a friend of a friend ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "song_list.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Song titles to remove dashes and capitalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "231791"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_song_names(titles_list):\n",
    "    no_dot = list(map(lambda item: item.replace(\".\", \"\"), titles_list))\n",
    "    no_dash = list(map(lambda item: item.replace(\"-\", \" \"), no_dot))\n",
    "    return list(map(lambda item: string.capwords(item), no_dash))\n",
    "\n",
    "titles_list = song_list.song\n",
    "final_titles = clean_song_names(titles_list)\n",
    "len(final_titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Append Clean Song names to DF as a new column and drop old column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = song_list\n",
    "# new_df.head()\n",
    "new_titles = final_titles\n",
    "\n",
    "\n",
    "new_lyr = pd.DataFrame(new_titles)\n",
    "new_lyr.tail()\n",
    "\n",
    "final_df = new_df.join(new_lyr)\n",
    "\n",
    "final_df.drop(columns = ['song', 'level_0', \"index\"], axis = 1, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(231791, 5)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "year      0\n",
       "artist    0\n",
       "genre     0\n",
       "lyrics    0\n",
       "song      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.rename(columns = {0: \"song\"}, inplace = True)\n",
    "final_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(231791, 5)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Song titles with Nan Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Rock          108992\n",
       "Pop            40466\n",
       "Hip-Hop        24850\n",
       "Metal          23759\n",
       "Country        14387\n",
       "Jazz            7970\n",
       "Electronic      7966\n",
       "R&B             3401\n",
       "Name: genre, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.genre.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rock_df = final_df[final_df.genre == \"Rock\"][:2000]\n",
    "pop_df = final_df[final_df.genre == \"Pop\"][:2000]\n",
    "hip_df = final_df[final_df.genre == \"Hip-Hop\"][:2000]\n",
    "metal_df = final_df[final_df.genre == \"Metal\"][:2000]\n",
    "jazz_df = final_df[final_df.genre == \"Jazz\"][:2000]\n",
    "elec_df = final_df[final_df.genre == \"Electronic\"][:2000]\n",
    "country_df = final_df[final_df.genre == \"Country\"][:2000]\n",
    "rnb_df = final_df[final_df.genre == \"R&B\"][:2000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>artist</th>\n",
       "      <th>genre</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>song</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>2004</td>\n",
       "      <td>borialis</td>\n",
       "      <td>Rock</td>\n",
       "      <td>A lot of cats are hatin', slandering makin' ba...</td>\n",
       "      <td>Hourglass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>2004</td>\n",
       "      <td>borialis</td>\n",
       "      <td>Rock</td>\n",
       "      <td>Somebody tell me why we landed here on the pla...</td>\n",
       "      <td>Why Oh Why</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>2004</td>\n",
       "      <td>borialis</td>\n",
       "      <td>Rock</td>\n",
       "      <td>I'm spittin' with the venom\\nTo your soul thro...</td>\n",
       "      <td>Mightier Than The Sword</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>2004</td>\n",
       "      <td>borialis</td>\n",
       "      <td>Rock</td>\n",
       "      <td>Where should I begin cripplin' all you villain...</td>\n",
       "      <td>White Trash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>2004</td>\n",
       "      <td>borialis</td>\n",
       "      <td>Rock</td>\n",
       "      <td>Enough of all that, let's switch up the format...</td>\n",
       "      <td>Don T Mean A Thing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     year    artist genre                                             lyrics  \\\n",
       "345  2004  borialis  Rock  A lot of cats are hatin', slandering makin' ba...   \n",
       "346  2004  borialis  Rock  Somebody tell me why we landed here on the pla...   \n",
       "347  2004  borialis  Rock  I'm spittin' with the venom\\nTo your soul thro...   \n",
       "348  2004  borialis  Rock  Where should I begin cripplin' all you villain...   \n",
       "349  2004  borialis  Rock  Enough of all that, let's switch up the format...   \n",
       "\n",
       "                        song  \n",
       "345                Hourglass  \n",
       "346               Why Oh Why  \n",
       "347  Mightier Than The Sword  \n",
       "348              White Trash  \n",
       "349       Don T Mean A Thing  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rock_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_df.drop(final_df[final_df.genre == \"Rock\"].index, inplace = True)\n",
    "final_df.drop(final_df[final_df.genre == \"Pop\"].index, inplace = True)\n",
    "final_df.drop(final_df[final_df.genre == \"Hip-Hop\"].index, inplace = True)\n",
    "final_df.drop(final_df[final_df.genre == \"Metal\"].index, inplace = True)\n",
    "final_df.drop(final_df[final_df.genre == \"Jazz\"].index, inplace = True)\n",
    "final_df.drop(final_df[final_df.genre == \"Electronic\"].index, inplace = True)\n",
    "final_df.drop(final_df[final_df.genre == \"Country\"].index, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "R&B           3401\n",
       "Pop           2000\n",
       "Metal         2000\n",
       "Rock          2000\n",
       "Country       2000\n",
       "Jazz          2000\n",
       "Hip-Hop       2000\n",
       "Electronic    2000\n",
       "Name: genre, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maybe_df = final_df.append([rock_df, pop_df, hip_df, metal_df, jazz_df, elec_df, country_df])\n",
    "maybe_df.genre.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "maybe_df.drop(maybe_df[maybe_df.genre == \"R&B\"].index, inplace = True)\n",
    "maybe_df = maybe_df.append([rnb_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pop           2000\n",
       "Metal         2000\n",
       "Rock          2000\n",
       "Country       2000\n",
       "Jazz          2000\n",
       "R&B           2000\n",
       "Hip-Hop       2000\n",
       "Electronic    2000\n",
       "Name: genre, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maybe_df.genre.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "maybe_df.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "maybe_df.drop(columns = [\"index\"], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>artist</th>\n",
       "      <th>genre</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>song</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2004</td>\n",
       "      <td>borialis</td>\n",
       "      <td>Rock</td>\n",
       "      <td>A lot of cats are hatin', slandering makin' ba...</td>\n",
       "      <td>Hourglass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2004</td>\n",
       "      <td>borialis</td>\n",
       "      <td>Rock</td>\n",
       "      <td>Somebody tell me why we landed here on the pla...</td>\n",
       "      <td>Why Oh Why</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2004</td>\n",
       "      <td>borialis</td>\n",
       "      <td>Rock</td>\n",
       "      <td>I'm spittin' with the venom\\nTo your soul thro...</td>\n",
       "      <td>Mightier Than The Sword</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2004</td>\n",
       "      <td>borialis</td>\n",
       "      <td>Rock</td>\n",
       "      <td>Where should I begin cripplin' all you villain...</td>\n",
       "      <td>White Trash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2004</td>\n",
       "      <td>borialis</td>\n",
       "      <td>Rock</td>\n",
       "      <td>Enough of all that, let's switch up the format...</td>\n",
       "      <td>Don T Mean A Thing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year    artist genre                                             lyrics  \\\n",
       "0  2004  borialis  Rock  A lot of cats are hatin', slandering makin' ba...   \n",
       "1  2004  borialis  Rock  Somebody tell me why we landed here on the pla...   \n",
       "2  2004  borialis  Rock  I'm spittin' with the venom\\nTo your soul thro...   \n",
       "3  2004  borialis  Rock  Where should I begin cripplin' all you villain...   \n",
       "4  2004  borialis  Rock  Enough of all that, let's switch up the format...   \n",
       "\n",
       "                      song  \n",
       "0                Hourglass  \n",
       "1               Why Oh Why  \n",
       "2  Mightier Than The Sword  \n",
       "3              White Trash  \n",
       "4       Don T Mean A Thing  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maybe_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_list = list(maybe_df.lyrics)\n",
    "\n",
    "pattern = \"([a-zA-Z]+(?:'[a-z]+)?)\"\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "english = list(set(nltk.corpus.words.words()))\n",
    "\n",
    "def clean_docs_lemma(lyrics_list):\n",
    "    cleaned = []\n",
    "    pattern = \"([a-zA-Z]+(?:'[a-z]+)?)\"\n",
    "    for lyric in lyrics_list:\n",
    "        clean_lyric = nltk.regexp_tokenize(lyric, pattern)\n",
    "        lyric_lower = [i.lower() for i in clean_lyric]\n",
    "        stop_words = stopwords.words('english')\n",
    "        stopwords_list = stop_words + list(string.punctuation)\n",
    "        stopwords_list += [\"''\", '\"\"', '...', '``']\n",
    "        lyrics_tokens_stopped = [w for w in lyric_lower if not w in stopwords_list]\n",
    "        lyric_lemmas = [lemmatizer.lemmatize(word) for word in lyrics_tokens_stopped]\n",
    "        c = \" \".join(lyric_lemmas)\n",
    "        cleaned.append(c)\n",
    "    return cleaned\n",
    "\n",
    "def clean_docs_stemma(lyrics_list):\n",
    "    cleaned = []\n",
    "    pattern = \"([a-zA-Z]+(?:'[a-z]+)?)\"\n",
    "    for lyric in lyrics_list:\n",
    "        clean_lyric = nltk.regexp_tokenize(lyric, pattern)\n",
    "        lyric_lower = [i.lower() for i in clean_lyric]\n",
    "        stop_words = stopwords.words('english')\n",
    "        stopwords_list = stop_words + list(string.punctuation)\n",
    "        stopwords_list += [\"''\", '\"\"', '...', '``']\n",
    "        lyrics_tokens_stopped = [w for w in lyric_lower if not w in stopwords_list]\n",
    "        lyric_stemmas = [stemmer.stem(word) for word in lyrics_tokens_stopped]\n",
    "        c = \" \".join(lyric_stemmas)\n",
    "        cleaned.append(c)\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First thing we wanted to do is test whether Lemmatizing works better than Stemmatizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmed Lyrics Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmed_lyrics = clean_docs_lemma(lyrics_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62165"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(nltk.word_tokenize(\" \".join(lemmed_lyrics))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = lemmed_lyrics\n",
    "y = maybe_df.genre\n",
    "\n",
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 8000)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Split Data in 3 pieces\n",
    "from sklearn.model_selection import train_test_split  \n",
    "X1, X2, y1, y2 = train_test_split(X, y, test_size=0.5, random_state=18)\n",
    "        \n",
    "len(y1), len(y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split  \n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.2, random_state=18) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Round 1 - Train 3 weakest models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "     steps=[('count_vectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      " ...obs=None,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False))])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "     steps=[('count_vectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      " ...m='SAMME.R', base_estimator=None,\n",
      "          learning_rate=0.3, n_estimators=50, random_state=None))])\n",
      "Pipeline(memory=None,\n",
      "     steps=[('count_vectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      " ...ki',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "           weights='uniform'))])\n",
      "Random Forest pipeline test accuracy: 0.367\n",
      "ADA pipeline test accuracy: 0.348\n",
      "KNN pipeline test accuracy: 0.151\n"
     ]
    }
   ],
   "source": [
    "# Train Weakest Models\n",
    "\n",
    "pipe_RF = Pipeline([('count_vectorizer', CountVectorizer()), \n",
    "                     ('tfidf_vectorizer', TfidfTransformer()),\n",
    "                     ('clf', RandomForestClassifier())\n",
    "                    ])\n",
    "\n",
    "pipe_ADA = Pipeline([('count_vectorizer', CountVectorizer()), \n",
    "                     ('tfidf_vectorizer', TfidfTransformer()),\n",
    "                     ('clf', AdaBoostClassifier(learning_rate=0.3))\n",
    "                    ])\n",
    "\n",
    "\n",
    "pipe_KNN = Pipeline([('count_vectorizer', CountVectorizer()), \n",
    "                     ('tfidf_vectorizer', TfidfTransformer()),\n",
    "                     ('clf', KNeighborsClassifier())\n",
    "                    ])\n",
    "\n",
    "# List of pipelines, List of pipeline names\n",
    "pipelines = [pipe_RF, pipe_ADA, pipe_KNN]\n",
    "pipeline_names = ['Random Forest', 'ADA', \"KNN\"]\n",
    "\n",
    "# Loop to fit each of the three pipelines\n",
    "for pipe in pipelines:\n",
    "    print(pipe)\n",
    "    pipe.fit(X1_train, y1_train)\n",
    "\n",
    "# Compare accuracies\n",
    "X1_scores = []\n",
    "for index, val in enumerate(pipelines):\n",
    "    tup = (pipeline_names[index], val.score(X1_test, y1_test), val.predict_proba(X1_train), val.predict(X1_train))\n",
    "    X1_scores.append(tup)\n",
    "    print('%s pipeline test accuracy: %.3f' % (pipeline_names[index], val.score(X1_test, y1_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Random Forest',\n",
       " 0.3675,\n",
       " array([[0.1       , 0.1       , 0.        , ..., 0.8       , 0.        ,\n",
       "         0.        ],\n",
       "        [0.1       , 0.        , 0.8       , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.1       , 0.1       , ..., 0.        , 0.        ,\n",
       "         0.1       ],\n",
       "        ...,\n",
       "        [0.6       , 0.1       , 0.        , ..., 0.2       , 0.        ,\n",
       "         0.        ],\n",
       "        [0.1       , 0.5       , 0.        , ..., 0.05833333, 0.        ,\n",
       "         0.1       ],\n",
       "        [0.8       , 0.1       , 0.        , ..., 0.1       , 0.        ,\n",
       "         0.        ]]),\n",
       " array(['Pop', 'Hip-Hop', 'Metal', ..., 'Country', 'Electronic', 'Country'],\n",
       "       dtype=object))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1_scores[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 792x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X1_models = [item[0] for item in X1_scores]\n",
    "X1_accuracy = [item[1] for item in X1_scores]\n",
    "\n",
    "plt.figure(figsize=(11,7))\n",
    "plt.title(\"Basic Model Scores for Round_1 Data\")\n",
    "plt.xlabel(\"Model\", fontsize = 14)\n",
    "plt.ylabel(\"Accuracy Score\", fontsize = 14)\n",
    "plt.bar(X1_models, X1_accuracy, label = \"{}  {}  {}\".format(round(X1_scores[0][1], 2), round(X1_scores[1][1], 2), round(X1_scores[2][1], 2)))\n",
    "\n",
    "plt.axhline(y=1/len(set(y)), color='#17CA83', linestyle='-', label = \"Random Guessing\")\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Creating Ensemble of 3 weakest learners - NB, Adaboost and KNN\n",
    "classes = val.classes_\n",
    "classes\n",
    "\n",
    "\n",
    "R1_AVG_Scores = (X1_scores[0][2] + X1_scores[1][2] + X1_scores[2][2])/3\n",
    "R1_df =  pd.DataFrame(R1_AVG_Scores, columns = [item + \"_AVG\" for item in classes])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split  \n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.2, random_state=18) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Round 2 Train Good learners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "     steps=[('count_vectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      " ...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])\n",
      "Pipeline(memory=None,\n",
      "     steps=[('count_vectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      " ...    subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
      "              verbose=0, warm_start=False))])\n",
      "Multinomail NB pipeline test accuracy: 0.415\n",
      "Gboost pipeline test accuracy: 0.414\n"
     ]
    }
   ],
   "source": [
    "pipe_NB = Pipeline([('count_vectorizer', CountVectorizer()), \n",
    "                     ('tfidf_vectorizer', TfidfTransformer()),\n",
    "                     ('clf', MultinomialNB())\n",
    "                    ])\n",
    "\n",
    "pipe_GBoost = Pipeline([('count_vectorizer', CountVectorizer()), \n",
    "                     ('tfidf_vectorizer', TfidfTransformer()),\n",
    "                     ('clf', GradientBoostingClassifier(learning_rate=0.3))\n",
    "                    ])\n",
    "\n",
    "# List of pipelines, List of pipeline names\n",
    "pipelines = [pipe_NB, pipe_GBoost]\n",
    "pipeline_names = ['Multinomail NB', \"Gboost\"]\n",
    "\n",
    "# Loop to fit each of the three pipelines\n",
    "for pipe in pipelines:\n",
    "    print(pipe)\n",
    "    pipe.fit(X2_train, y2_train)\n",
    "\n",
    "# Compare accuracies\n",
    "X2_scores = []\n",
    "for index, val in enumerate(pipelines):\n",
    "    tup = (pipeline_names[index], val.score(X2_test, y2_test), val.predict_proba(X2_train), val.predict(X2_train))\n",
    "    X2_scores.append(tup)\n",
    "    print('%s pipeline test accuracy: %.3f' % (pipeline_names[index], val.score(X2_test, y2_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Multinomail NB',\n",
       " 0.415,\n",
       " array([[0.06476146, 0.35241613, 0.06331353, ..., 0.13243585, 0.08613732,\n",
       "         0.10548437],\n",
       "        [0.1981774 , 0.07245812, 0.04322267, ..., 0.10622388, 0.18648187,\n",
       "         0.16461711],\n",
       "        [0.22100511, 0.08076004, 0.0505977 , ..., 0.12314752, 0.11177447,\n",
       "         0.0915945 ],\n",
       "        ...,\n",
       "        [0.19471283, 0.07157141, 0.03604731, ..., 0.09726023, 0.15274023,\n",
       "         0.16414313],\n",
       "        [0.11403141, 0.18961607, 0.15143814, ..., 0.09237297, 0.09056385,\n",
       "         0.10338478],\n",
       "        [0.0982226 , 0.19033551, 0.10705505, ..., 0.13630554, 0.1378852 ,\n",
       "         0.10853184]]),\n",
       " array(['Electronic', 'Country', 'Jazz', ..., 'Country', 'Electronic',\n",
       "        'Electronic'], dtype='<U10'))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2_scores[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "R2_AVG_Scores = (X2_scores[0][2] + X2_scores[1][2])/2\n",
    "R2_df =  pd.DataFrame(R2_AVG_Scores, columns = [item + \"_AVG\" for item in classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R1_df.shape == R2_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country_AVG</th>\n",
       "      <th>Electronic_AVG</th>\n",
       "      <th>Hip-Hop_AVG</th>\n",
       "      <th>Jazz_AVG</th>\n",
       "      <th>Metal_AVG</th>\n",
       "      <th>Pop_AVG</th>\n",
       "      <th>R&amp;B_AVG</th>\n",
       "      <th>Rock_AVG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.021688</td>\n",
       "      <td>0.077889</td>\n",
       "      <td>0.047074</td>\n",
       "      <td>0.077984</td>\n",
       "      <td>0.172722</td>\n",
       "      <td>0.381309</td>\n",
       "      <td>0.145245</td>\n",
       "      <td>0.076089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.014405</td>\n",
       "      <td>0.123191</td>\n",
       "      <td>0.404214</td>\n",
       "      <td>0.014668</td>\n",
       "      <td>0.093370</td>\n",
       "      <td>0.258538</td>\n",
       "      <td>0.061224</td>\n",
       "      <td>0.030391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.042194</td>\n",
       "      <td>0.141628</td>\n",
       "      <td>0.039385</td>\n",
       "      <td>0.042362</td>\n",
       "      <td>0.440424</td>\n",
       "      <td>0.175590</td>\n",
       "      <td>0.075555</td>\n",
       "      <td>0.042863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.074281</td>\n",
       "      <td>0.109367</td>\n",
       "      <td>0.072252</td>\n",
       "      <td>0.041520</td>\n",
       "      <td>0.108542</td>\n",
       "      <td>0.208857</td>\n",
       "      <td>0.042202</td>\n",
       "      <td>0.342980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.276824</td>\n",
       "      <td>0.074027</td>\n",
       "      <td>0.037418</td>\n",
       "      <td>0.077080</td>\n",
       "      <td>0.273547</td>\n",
       "      <td>0.176094</td>\n",
       "      <td>0.042204</td>\n",
       "      <td>0.042805</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Country_AVG  Electronic_AVG  Hip-Hop_AVG  Jazz_AVG  Metal_AVG   Pop_AVG  \\\n",
       "0     0.021688        0.077889     0.047074  0.077984   0.172722  0.381309   \n",
       "1     0.014405        0.123191     0.404214  0.014668   0.093370  0.258538   \n",
       "2     0.042194        0.141628     0.039385  0.042362   0.440424  0.175590   \n",
       "3     0.074281        0.109367     0.072252  0.041520   0.108542  0.208857   \n",
       "4     0.276824        0.074027     0.037418  0.077080   0.273547  0.176094   \n",
       "\n",
       "    R&B_AVG  Rock_AVG  \n",
       "0  0.145245  0.076089  \n",
       "1  0.061224  0.030391  \n",
       "2  0.075555  0.042863  \n",
       "3  0.042202  0.342980  \n",
       "4  0.042204  0.042805  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R1_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country_AVG</th>\n",
       "      <th>Electronic_AVG</th>\n",
       "      <th>Hip-Hop_AVG</th>\n",
       "      <th>Jazz_AVG</th>\n",
       "      <th>Metal_AVG</th>\n",
       "      <th>Pop_AVG</th>\n",
       "      <th>R&amp;B_AVG</th>\n",
       "      <th>Rock_AVG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.047782</td>\n",
       "      <td>0.517785</td>\n",
       "      <td>0.035567</td>\n",
       "      <td>0.081856</td>\n",
       "      <td>0.058145</td>\n",
       "      <td>0.099964</td>\n",
       "      <td>0.065361</td>\n",
       "      <td>0.093540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.137993</td>\n",
       "      <td>0.057003</td>\n",
       "      <td>0.025317</td>\n",
       "      <td>0.192387</td>\n",
       "      <td>0.048993</td>\n",
       "      <td>0.110806</td>\n",
       "      <td>0.122537</td>\n",
       "      <td>0.304965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.136361</td>\n",
       "      <td>0.068976</td>\n",
       "      <td>0.029945</td>\n",
       "      <td>0.392138</td>\n",
       "      <td>0.096692</td>\n",
       "      <td>0.101011</td>\n",
       "      <td>0.080609</td>\n",
       "      <td>0.094267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.175803</td>\n",
       "      <td>0.081353</td>\n",
       "      <td>0.024063</td>\n",
       "      <td>0.201832</td>\n",
       "      <td>0.086792</td>\n",
       "      <td>0.206319</td>\n",
       "      <td>0.113154</td>\n",
       "      <td>0.110683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.066228</td>\n",
       "      <td>0.065306</td>\n",
       "      <td>0.043323</td>\n",
       "      <td>0.045614</td>\n",
       "      <td>0.512806</td>\n",
       "      <td>0.090862</td>\n",
       "      <td>0.072373</td>\n",
       "      <td>0.103488</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Country_AVG  Electronic_AVG  Hip-Hop_AVG  Jazz_AVG  Metal_AVG   Pop_AVG  \\\n",
       "0     0.047782        0.517785     0.035567  0.081856   0.058145  0.099964   \n",
       "1     0.137993        0.057003     0.025317  0.192387   0.048993  0.110806   \n",
       "2     0.136361        0.068976     0.029945  0.392138   0.096692  0.101011   \n",
       "3     0.175803        0.081353     0.024063  0.201832   0.086792  0.206319   \n",
       "4     0.066228        0.065306     0.043323  0.045614   0.512806  0.090862   \n",
       "\n",
       "    R&B_AVG  Rock_AVG  \n",
       "0  0.065361  0.093540  \n",
       "1  0.122537  0.304965  \n",
       "2  0.080609  0.094267  \n",
       "3  0.113154  0.110683  \n",
       "4  0.072373  0.103488  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R2_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "R3_df = pd.concat([R1_df, R2_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "y3 = pd.concat([y1_train, y2_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12800,)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6400, 6400)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y3[0:6400] == y1_train), sum(y3[6400:] == y2_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Round 3 - Plug all AVG probabilities as features and train a final NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn import preprocessing\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original class labels:\n",
      "['Country', 'Electronic', 'Hip-Hop', 'Jazz', 'Metal', 'Pop', 'R&B', 'Rock']\n",
      "\n",
      "\n",
      "New product labels:\n",
      "[5 2 4 ... 4 1 1]\n",
      "\n",
      "\n",
      "One hot labels; 7 binary columns, one for each of the categories.\n",
      "[[0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]]\n",
      "\n",
      "\n",
      "One hot labels shape:\n",
      "(12800, 8)\n"
     ]
    }
   ],
   "source": [
    "#Converting y_test to categorical\n",
    "\n",
    "product = y3\n",
    "\n",
    "le = preprocessing.LabelEncoder() #Initialize. le used as abbreviation fo label encoder\n",
    "le.fit(product)\n",
    "print(\"Original class labels:\")\n",
    "print(list(le.classes_))\n",
    "print('\\n')\n",
    "product_cat = le.transform(product)  \n",
    "#list(le.inverse_transform([0, 1, 3, 3, 0, 6, 4])) #If you wish to retrieve the original descriptive labels post production\n",
    "\n",
    "print('New product labels:')\n",
    "print(product_cat)\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "print('One hot labels; 7 binary columns, one for each of the categories.') #Each row will be all zeros except for the category for that observation.\n",
    "product_onehot = to_categorical(product_cat)\n",
    "print(product_onehot)\n",
    "print('\\n')\n",
    "\n",
    "print('One hot labels shape:')\n",
    "print(np.shape(product_onehot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7200, 8) (1800, 8) (7200, 8) (1800, 8)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X3_train, X3_test, y3_train, y3_test = train_test_split(R3_df[:9000], product_onehot[:9000], test_size=0.2, random_state=123)  \n",
    "\n",
    "# X_train = X_train.reset_index(drop=True)\n",
    "# y_train = y_train.reset_index(drop=True)  \n",
    "\n",
    "\n",
    "print(X3_train.shape, X3_test.shape, y3_train.shape, y3_test.shape)\n",
    "y3_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate NN mmodel\n",
    "\n",
    "random.seed(123)\n",
    "from keras import models\n",
    "from keras import layers\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(7, input_dim=8, kernel_initializer='normal', activation='tanh')) #2 hidden layers\n",
    "model.add(layers.Dense(3, activation='tanh'))\n",
    "model.add(layers.Dense(8, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7200 samples, validate on 1800 samples\n",
      "Epoch 1/50\n",
      "7200/7200 [==============================] - 1s 71us/step - loss: 2.0675 - acc: 0.3221 - val_loss: 2.0618 - val_acc: 0.4339\n",
      "Epoch 2/50\n",
      "7200/7200 [==============================] - 0s 25us/step - loss: 2.0555 - acc: 0.4379 - val_loss: 2.0497 - val_acc: 0.4450\n",
      "Epoch 3/50\n",
      "7200/7200 [==============================] - 0s 30us/step - loss: 2.0428 - acc: 0.4042 - val_loss: 2.0366 - val_acc: 0.4589\n",
      "Epoch 4/50\n",
      "7200/7200 [==============================] - 0s 29us/step - loss: 2.0287 - acc: 0.4597 - val_loss: 2.0220 - val_acc: 0.4583\n",
      "Epoch 5/50\n",
      "7200/7200 [==============================] - 0s 34us/step - loss: 2.0127 - acc: 0.4426 - val_loss: 2.0051 - val_acc: 0.4567\n",
      "Epoch 6/50\n",
      "7200/7200 [==============================] - 0s 28us/step - loss: 1.9941 - acc: 0.4535 - val_loss: 1.9854 - val_acc: 0.4511\n",
      "Epoch 7/50\n",
      "7200/7200 [==============================] - 0s 24us/step - loss: 1.9724 - acc: 0.4486 - val_loss: 1.9622 - val_acc: 0.4411\n",
      "Epoch 8/50\n",
      "7200/7200 [==============================] - 0s 25us/step - loss: 1.9470 - acc: 0.4426 - val_loss: 1.9350 - val_acc: 0.4356\n",
      "Epoch 9/50\n",
      "7200/7200 [==============================] - 0s 25us/step - loss: 1.9175 - acc: 0.4351 - val_loss: 1.9038 - val_acc: 0.4450\n",
      "Epoch 10/50\n",
      "7200/7200 [==============================] - 0s 26us/step - loss: 1.8837 - acc: 0.4460 - val_loss: 1.8681 - val_acc: 0.4367\n",
      "Epoch 11/50\n",
      "7200/7200 [==============================] - 0s 26us/step - loss: 1.8459 - acc: 0.4344 - val_loss: 1.8289 - val_acc: 0.4406\n",
      "Epoch 12/50\n",
      "7200/7200 [==============================] - 0s 26us/step - loss: 1.8048 - acc: 0.4425 - val_loss: 1.7866 - val_acc: 0.4444\n",
      "Epoch 13/50\n",
      "7200/7200 [==============================] - 0s 25us/step - loss: 1.7612 - acc: 0.4546 - val_loss: 1.7422 - val_acc: 0.4467\n",
      "Epoch 14/50\n",
      "7200/7200 [==============================] - 0s 25us/step - loss: 1.7163 - acc: 0.4583 - val_loss: 1.6968 - val_acc: 0.4567\n",
      "Epoch 15/50\n",
      "7200/7200 [==============================] - ETA: 0s - loss: 1.6732 - acc: 0.468 - 0s 28us/step - loss: 1.6709 - acc: 0.4686 - val_loss: 1.6514 - val_acc: 0.4633\n",
      "Epoch 16/50\n",
      "7200/7200 [==============================] - 0s 24us/step - loss: 1.6254 - acc: 0.4821 - val_loss: 1.6060 - val_acc: 0.4911\n",
      "Epoch 17/50\n",
      "7200/7200 [==============================] - 0s 25us/step - loss: 1.5798 - acc: 0.5074 - val_loss: 1.5604 - val_acc: 0.5217\n",
      "Epoch 18/50\n",
      "7200/7200 [==============================] - 0s 26us/step - loss: 1.5342 - acc: 0.5515 - val_loss: 1.5148 - val_acc: 0.5844\n",
      "Epoch 19/50\n",
      "7200/7200 [==============================] - 0s 27us/step - loss: 1.4884 - acc: 0.5943 - val_loss: 1.4694 - val_acc: 0.6150\n",
      "Epoch 20/50\n",
      "7200/7200 [==============================] - 0s 26us/step - loss: 1.4427 - acc: 0.6208 - val_loss: 1.4245 - val_acc: 0.6367\n",
      "Epoch 21/50\n",
      "7200/7200 [==============================] - 0s 27us/step - loss: 1.3974 - acc: 0.6415 - val_loss: 1.3793 - val_acc: 0.6583\n",
      "Epoch 22/50\n",
      "7200/7200 [==============================] - 0s 29us/step - loss: 1.3528 - acc: 0.6560 - val_loss: 1.3359 - val_acc: 0.6706\n",
      "Epoch 23/50\n",
      "7200/7200 [==============================] - 0s 32us/step - loss: 1.3097 - acc: 0.6725 - val_loss: 1.2947 - val_acc: 0.6672\n",
      "Epoch 24/50\n",
      "7200/7200 [==============================] - 0s 28us/step - loss: 1.2686 - acc: 0.6806 - val_loss: 1.2551 - val_acc: 0.6944\n",
      "Epoch 25/50\n",
      "7200/7200 [==============================] - 0s 26us/step - loss: 1.2300 - acc: 0.6907 - val_loss: 1.2177 - val_acc: 0.6917\n",
      "Epoch 26/50\n",
      "7200/7200 [==============================] - 0s 26us/step - loss: 1.1935 - acc: 0.6951 - val_loss: 1.1827 - val_acc: 0.7100\n",
      "Epoch 27/50\n",
      "7200/7200 [==============================] - 0s 25us/step - loss: 1.1593 - acc: 0.7062 - val_loss: 1.1507 - val_acc: 0.7161\n",
      "Epoch 28/50\n",
      "7200/7200 [==============================] - 0s 27us/step - loss: 1.1275 - acc: 0.7164 - val_loss: 1.1200 - val_acc: 0.7333\n",
      "Epoch 29/50\n",
      "7200/7200 [==============================] - 0s 27us/step - loss: 1.0976 - acc: 0.7236 - val_loss: 1.0912 - val_acc: 0.7383\n",
      "Epoch 30/50\n",
      "7200/7200 [==============================] - 0s 30us/step - loss: 1.0696 - acc: 0.7325 - val_loss: 1.0647 - val_acc: 0.7433\n",
      "Epoch 31/50\n",
      "7200/7200 [==============================] - 0s 33us/step - loss: 1.0432 - acc: 0.7400 - val_loss: 1.0392 - val_acc: 0.7533\n",
      "Epoch 32/50\n",
      "7200/7200 [==============================] - 0s 27us/step - loss: 1.0185 - acc: 0.7478 - val_loss: 1.0152 - val_acc: 0.7606\n",
      "Epoch 33/50\n",
      "7200/7200 [==============================] - 0s 29us/step - loss: 0.9949 - acc: 0.7601 - val_loss: 0.9934 - val_acc: 0.7656\n",
      "Epoch 34/50\n",
      "7200/7200 [==============================] - 0s 32us/step - loss: 0.9724 - acc: 0.7668 - val_loss: 0.9716 - val_acc: 0.7750\n",
      "Epoch 35/50\n",
      "7200/7200 [==============================] - 0s 33us/step - loss: 0.9514 - acc: 0.7746 - val_loss: 0.9510 - val_acc: 0.7833\n",
      "Epoch 36/50\n",
      "7200/7200 [==============================] - 0s 34us/step - loss: 0.9314 - acc: 0.7828 - val_loss: 0.9319 - val_acc: 0.7878\n",
      "Epoch 37/50\n",
      "7200/7200 [==============================] - 0s 31us/step - loss: 0.9120 - acc: 0.7875 - val_loss: 0.9131 - val_acc: 0.7917\n",
      "Epoch 38/50\n",
      "7200/7200 [==============================] - 0s 33us/step - loss: 0.8935 - acc: 0.7931 - val_loss: 0.8956 - val_acc: 0.7978\n",
      "Epoch 39/50\n",
      "7200/7200 [==============================] - 0s 33us/step - loss: 0.8761 - acc: 0.7969 - val_loss: 0.8786 - val_acc: 0.8000\n",
      "Epoch 40/50\n",
      "7200/7200 [==============================] - 0s 32us/step - loss: 0.8593 - acc: 0.8004 - val_loss: 0.8631 - val_acc: 0.8022\n",
      "Epoch 41/50\n",
      "7200/7200 [==============================] - 0s 34us/step - loss: 0.8435 - acc: 0.8060 - val_loss: 0.8481 - val_acc: 0.8089\n",
      "Epoch 42/50\n",
      "7200/7200 [==============================] - 0s 25us/step - loss: 0.8282 - acc: 0.8058 - val_loss: 0.8327 - val_acc: 0.8111\n",
      "Epoch 43/50\n",
      "7200/7200 [==============================] - 0s 34us/step - loss: 0.8135 - acc: 0.8075 - val_loss: 0.8187 - val_acc: 0.8133\n",
      "Epoch 44/50\n",
      "7200/7200 [==============================] - 0s 31us/step - loss: 0.7994 - acc: 0.8108 - val_loss: 0.8074 - val_acc: 0.8200\n",
      "Epoch 45/50\n",
      "7200/7200 [==============================] - 0s 25us/step - loss: 0.7861 - acc: 0.8157 - val_loss: 0.7926 - val_acc: 0.8200\n",
      "Epoch 46/50\n",
      "7200/7200 [==============================] - 0s 25us/step - loss: 0.7731 - acc: 0.8186 - val_loss: 0.7806 - val_acc: 0.8194\n",
      "Epoch 47/50\n",
      "7200/7200 [==============================] - 0s 26us/step - loss: 0.7608 - acc: 0.8203 - val_loss: 0.7687 - val_acc: 0.8228\n",
      "Epoch 48/50\n",
      "7200/7200 [==============================] - 0s 25us/step - loss: 0.7489 - acc: 0.8251 - val_loss: 0.7577 - val_acc: 0.8250\n",
      "Epoch 49/50\n",
      "7200/7200 [==============================] - 0s 26us/step - loss: 0.7378 - acc: 0.8246 - val_loss: 0.7482 - val_acc: 0.8328\n",
      "Epoch 50/50\n",
      "7200/7200 [==============================] - 0s 25us/step - loss: 0.7270 - acc: 0.8282 - val_loss: 0.7367 - val_acc: 0.8283\n"
     ]
    }
   ],
   "source": [
    "model_val = model.fit(X3_train,\n",
    "                    y3_train,\n",
    "                    epochs=50,\n",
    "                    batch_size=48,\n",
    "                    validation_data=(X3_test, y3_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do final Test with 'Holdout' data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3800/3800 [==============================] - 0s 12us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7857534348337274, 0.8039473685465361]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_score = model.evaluate(R3_df[9000:], product_onehot[9000:])\n",
    "validation_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model.predict_classes(R3_df[9000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Electronic',\n",
       " 'R&B',\n",
       " 'Electronic',\n",
       " 'Electronic',\n",
       " 'Hip-Hop',\n",
       " 'Metal',\n",
       " 'Jazz',\n",
       " 'Pop',\n",
       " 'Electronic',\n",
       " 'Metal']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tryit = list(le.inverse_transform(y_hat))\n",
    "tryit[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 3, 1, 4, 6, 6, 4, 4, 1, 6])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 3, 1, 4, 6, 1, 4, 4, 1, 1]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual = [np.where(item == 1)[0][0] for item in product_onehot[9000:]]\n",
    "actual[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Rock',\n",
       " 'Jazz',\n",
       " 'Electronic',\n",
       " 'Metal',\n",
       " 'R&B',\n",
       " 'Electronic',\n",
       " 'Metal',\n",
       " 'Metal',\n",
       " 'Electronic',\n",
       " 'Electronic']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_genre = list(le.inverse_transform(actual))\n",
    "actual_genre[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "for item in list(zip(tryit, actual_genre)):\n",
    "        if item[0] == item[1]:\n",
    "            count += 1\n",
    "            \n",
    "count/len(tryit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_val_dict = model_val.history\n",
    "model_val_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_train = model.evaluate(X_train, y_train)\n",
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_test = model.evaluate(X_test, y_test)\n",
    "results_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_test = model.predict(X_test)\n",
    "result = y_test - y_hat_test\n",
    "print(sum(sum(result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split  \n",
    "X_train, X_test, y_train, y_test = train_test_split(lemmed_lyrics, maybe_df.genre, test_size=0.2, random_state=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modles_lem = [item[0] for item in lemmed_basic_scores]\n",
    "accuracy_lem = [item[1] for item in lemmed_basic_scores]\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from collections import namedtuple\n",
    "\n",
    "\n",
    "n_groups = 5\n",
    "\n",
    "means_men = (20, 35, 30, 35, 27)\n",
    "std_men = (2, 3, 4, 1, 2)\n",
    "\n",
    "means_women = (25, 32, 34, 20, 25)\n",
    "std_women = (3, 5, 2, 3, 3)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (12, 8))\n",
    "\n",
    "\n",
    "index = np.arange(n_groups)\n",
    "bar_width = 0.35\n",
    "\n",
    "opacity = 0.5\n",
    "error_config = {'ecolor': '0.3'}\n",
    "\n",
    "lemmed_bars = ax.bar(index, accuracy_lem, bar_width,\n",
    "                alpha=opacity, color='b',\n",
    "                label='Lematized')\n",
    "\n",
    "stemmed_bars = ax.bar(index + bar_width, accuracy_stem, bar_width,\n",
    "                alpha=opacity, color='r',\n",
    "                label='Stemmatized')\n",
    "\n",
    "ax.set_xlabel('Model Type', fontsize = 14)\n",
    "ax.set_ylabel('Accuracy Scores', fontsize = 14)\n",
    "ax.set_title('Stemmed vs. Lemmed Accuracy Score Comparison', fontsize = 18)\n",
    "ax.set_xticks(index + bar_width / 2)\n",
    "ax.set_xticklabels(modles_lem)\n",
    "\n",
    "plt.axhline(y=1/len(set(y)), color='#17CA83', linestyle='-', label = \"Random Guessing\")\n",
    "ax.legend()\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We decided to pick Lemmatized over Stemmatized and top three models for further optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next we want to try using PCA to improve performance and reduce dimentionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tfidf = TfidfVectorizer()\n",
    "# response = tfidf.fit_transform(lemmed_lyrics)\n",
    "\n",
    "# PCA_df = pd.DataFrame(response.toarray(), columns=tfidf.get_feature_names())\n",
    "# PCA_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA = response  # this comes from above where you're vectorizing tdif dictionary\n",
    "\n",
    "# non_zero_cols = DATA.nnz / float(DATA.shape[0])\n",
    "# print(\"Average Number of Non-Zero Elements in Vectorized Articles: {}\".format(non_zero_cols))\n",
    "\n",
    "# percent_sparse = 1 - (non_zero_cols / float(DATA.shape[1]))\n",
    "# print('Percentage of columns containing 0: {}'.format(percent_sparse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# PCA_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Features table and Target table and testing first model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# X_pca = PCA_df\n",
    "# y_pca = maybe_df.genre\n",
    "\n",
    "# len(X_pca) == len(y_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split  \n",
    "# X_train_pca, X_test_pca, y_train_pca, y_test_pca = train_test_split(X_pca, y_pca, test_size=0.2, random_state=18) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try different PCA values and pick a number that preserves sufficient % of variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA\n",
    "# pca_1 = PCA(n_components=500)\n",
    "# pca_2 = PCA(n_components=1000)\n",
    "# pca_3 = PCA(n_components=1455)\n",
    "# pca_4 = PCA(n_components=2000)\n",
    "\n",
    "# principalComponents = pca_1.fit_transform(X_pca)\n",
    "# principalComponents = pca_2.fit_transform(X_pca)\n",
    "# principalComponents = pca_3.fit_transform(X_pca)\n",
    "# principalComponents = pca_4.fit_transform(X_pca)\n",
    "\n",
    "# print(np.sum(pca_1.explained_variance_ratio_))\n",
    "# print(np.sum(pca_2.explained_variance_ratio_))\n",
    "# print(np.sum(pca_3.explained_variance_ratio_))\n",
    "# print(np.sum(pca_4.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will test PCA witn n = 1800 on our top 3 models to see if it helps performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pipe_NB_pca = Pipeline([('pca', PCA(n_components=3000, random_state=18)),\n",
    "#                      ('clf', GaussianNB())\n",
    "#                     ])\n",
    "\n",
    "# pipe_RF_pca = Pipeline([('pca', PCA(n_components=3000, random_state=18)),\n",
    "#                      ('clf', RandomForestClassifier(n_jobs = -1))\n",
    "#                    ])\n",
    "                  \n",
    "# pipe_GBoost_pca = Pipeline([('pca', PCA(n_components=1800, random_state=18)),\n",
    "#                      ('clf', GradientBoostingClassifier(learning_rate=0.3))\n",
    "#                     ])\n",
    "\n",
    "\n",
    "# # List of pipelines, List of pipeline names\n",
    "# pipelines = [pipe_NB_pca, pipe_RF_pca, pipe_GBoost_pca]\n",
    "# pipeline_names = ['Multinomial NB', \"Random Forest\", \"Gradient Boost\"]\n",
    "\n",
    "# # Loop to fit each of the three pipelines\n",
    "# for pipe in pipelines:\n",
    "#     print(pipe)\n",
    "#     pipe.fit(X_train_pca, y_train_pca)\n",
    "\n",
    "# # Compare accuracies\n",
    "# PCA_scores = []\n",
    "# for index, val in enumerate(pipelines):\n",
    "#     tup = (pipeline_names[index], val.score(X_test_pca, y_test_pca))\n",
    "#     lemmed_basic_scores.append(tup)\n",
    "#     print('%s pipeline test accuracy: %.3f' % (pipeline_names[index], val.score(X_test_pca, y_test_pca)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA1800_results = dict(Multinomail_NB = 0.203,\n",
    "# Gradient_boost = 0.422,\n",
    "# Random_forest =0.290)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# n_groups = 3\n",
    "\n",
    "# basic_mod_name = modles_lem[:3]\n",
    "# basic_mod_acc = accuracy_lem[:3]\n",
    "# pca_models_acc = [item[1] for item in PCA1800_results.items()]\n",
    "\n",
    "# fig, ax = plt.subplots(figsize = (10,8))\n",
    "\n",
    "\n",
    "# index = np.arange(n_groups)\n",
    "# bar_width = 0.35\n",
    "\n",
    "# opacity = 0.5\n",
    "# error_config = {'ecolor': '0.3'}\n",
    "\n",
    "# basic_bars = ax.bar(index, basic_mod_acc, bar_width,\n",
    "#                 alpha=opacity, color='b',\n",
    "#                 label='Basic Model')\n",
    "\n",
    "# pca_bars = ax.bar(index + bar_width, pca_models_acc , bar_width,\n",
    "#                 alpha=opacity, color='r',\n",
    "#                 label='PCA n_components = 1800')\n",
    "\n",
    "# ax.set_xlabel('Model Type', fontsize = 14)\n",
    "# ax.set_ylabel('Accuracy Scores', fontsize = 14)\n",
    "# ax.set_title('Basic Model vs. PCA with n = 1800 Model Comparison', fontsize = 18)\n",
    "# ax.set_xticks(index + bar_width / 2)\n",
    "# ax.set_xticklabels(basic_mod_name)\n",
    "\n",
    "# plt.axhline(y=1/len(set(y)), color='#17CA83', linestyle='-', label = \"Random Guessing\")\n",
    "# ax.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We decided that it's not worth using PCA for our models because it increases computational time and doesn't really improve performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's use GridSearch to try to optimize our  top 3 models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Top3_pipelines = [pipe_NB, pipe_GBoost, pipe_RF]\n",
    "Top3_pipeline_names = ['Multinomail NB', \"Gboost\", 'Random Forest']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "GS_pipe_RF = Pipeline([('count_vectorizer', CountVectorizer()), \n",
    "                     ('tfidf_vectorizer', TfidfTransformer()),\n",
    "                     ('clf', RandomForestClassifier())\n",
    "                    ])\n",
    "\n",
    "sorted(GS_pipe_RF.get_params().keys())\n",
    "\n",
    "rf_param_grid = dict(clf__n_estimators = [10, 30, 100], clf__criterion = ['gini', 'entropy'], \n",
    "                    clf__max_depth = [2, 6, 10], clf__min_samples_split = [5, 10],\n",
    "                    clf__min_samples_leaf = [3, 6])\n",
    "\n",
    "\n",
    "gs_RF = GridSearchCV(estimator=GS_pipe_RF,\n",
    "            param_grid=rf_param_grid,\n",
    "            scoring='accuracy',\n",
    "            cv=3)\n",
    "\n",
    "gs_RF.fit(X_train, y_train)\n",
    "\n",
    "dt_gs_training_score = np.mean(gs_RF.cv_results_['mean_train_score'])\n",
    "dt_gs_testing_score = gs_RF.score(X_test, y_test)\n",
    "\n",
    "print(\"Mean Training Score: {:.4}%\".format(dt_gs_training_score * 100))\n",
    "print(\"Mean Testing Score: {:.4}%\".format(dt_gs_testing_score * 100))\n",
    "print(\"Best Parameter Combination Found During Grid Search: {}\".format(gs_RF.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Grid_RF_metrics = dict(train_score = dt_gs_training_score, test_score = dt_gs_testing_score, best_params = gs_RF.best_params_)\n",
    "Grid_RF_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch Gboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "GS_pipe_GB = Pipeline([('count_vectorizer', CountVectorizer()), \n",
    "                     ('tfidf_vectorizer', TfidfTransformer()),\n",
    "                     ('clf', GradientBoostingClassifier())\n",
    "                    ])\n",
    "# sorted(GS_pipe_RF.get_params().keys())\n",
    "\n",
    "GB_params = {\n",
    "    \"clf__learning_rate\": [0.2, 0.25],\n",
    "    \"clf__min_samples_split\": [4, 5],\n",
    "    \"clf__min_samples_leaf\": [6],\n",
    "    \"clf__max_depth\":[3],\n",
    "    \"clf__n_estimators\":[100, 150]\n",
    "    }\n",
    "\n",
    "gs_GB = GridSearchCV(estimator=GS_pipe_GB,\n",
    "            param_grid=GB_params,\n",
    "            scoring='accuracy',\n",
    "            cv=3)\n",
    "\n",
    "gs_GB.fit(X_train, y_train)\n",
    "\n",
    "dt_GB_training_score = np.mean(gs_GB.cv_results_['mean_train_score'])\n",
    "dt_GB_testing_score = gs_GB.score(X_test, y_test)\n",
    "\n",
    "print(\"Mean Training Score: {:.4}%\".format(dt_GB_training_score * 100))\n",
    "print(\"Mean Testing Score: {:.4}%\".format(dt_GB_testing_score * 100))\n",
    "print(\"Best Parameter Combination Found During Grid Search: {}\".format(gs_GB.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Grid_GB_metrics = dict(train_score = dt_GB_training_score, test_score = dt_GB_testing_score, best_params = gs_GB.best_params_)\n",
    "Grid_GB_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hip_lyric = [\"dropin my dough real quick. Data Science squad for the win. Drake got nothin on us!\"]\n",
    "jazz_lyric = [\"humdinger, babababoo, bababaa from san francisco to georgia, we teach you to code like wah wah\"]\n",
    "rock_lyric = ['when I was young I thought code is not important. Now I learned that i need to know it if I want to grow.']\n",
    "def test_genre(lyric):\n",
    "    lemmed_test = clean_docs_lemma(lyric)\n",
    "    print(\"This song is definetely {}!\".format(gs_GB.predict(lemmed_test)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_genre(rock_lyric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_top3 = [(\"GradientBoost with GridSearch\", Grid_GB_metrics['test_score']), basic_scores[0], (\"Random Forest with GridSearch\", basic_scores[2][1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_top3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top3_model = [item[0] for item in final_top3]\n",
    "top3_scores = [item[1] for item in final_top3]\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.title(\"Top3 Models Final Performance\", color ='#061152' , fontsize = 20)\n",
    "plt.ylabel(\"Accuracy Score\", color = '#061152', fontsize = 16)\n",
    "plt.bar(top3_model, top3_scores, color = \"#17CA83\", label = \"Top 3 Models\")\n",
    "\n",
    "plt.axhline(y=1/len(set(y)), color='#AF2138', linestyle='-', label = \"Random Guessing\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_mode_df = pd.DataFrame.from_dict(Grid_GB_metrics['best_params'])\n",
    "top_mode_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim import corpora, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "test = random.sample(stemmed_lyrics, 7000)\n",
    "lemmed_lyr = [nltk.word_tokenize(doc) for doc in test]\n",
    "\n",
    "lemmed_lyr\n",
    "# test\n",
    "dictionary = gensim.corpora.Dictionary(lemmed_lyr)\n",
    "\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break\n",
    "\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n",
    "\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in lemmed_lyr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Topic Classifier using BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=15, id2word=dictionary, passes=2, workers=2)\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Topic Classifier Using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "\n",
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=15, id2word=dictionary, passes=2, workers=4)\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = tfidf[bow_corpus[12]]\n",
    "\n",
    "# Get terms from the dictionary and pair with weights\n",
    "\n",
    "weights = [(dictionary[pair[0]], pair[1]) for pair in weights]\n",
    "weights[-35:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install wordcloud\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Initialize the word cloud\n",
    "\n",
    "d = {}\n",
    "for a, x in weights:\n",
    "    d[a] = x\n",
    "    \n",
    "wc = WordCloud(\n",
    "    background_color=\"white\",\n",
    "    max_words=2000,\n",
    "    width = 1024,\n",
    "    height = 720,\n",
    "    stopwords=stopwords.words(\"english\")\n",
    ")\n",
    "\n",
    "# Generate the cloud\n",
    "\n",
    "wc.generate_from_frequencies(d)\n",
    "plt.figure(figsize = (12, 8))\n",
    "plt.imshow(wc, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
